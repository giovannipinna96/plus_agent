#!/usr/bin/env python3
"""
Multi-Agent Data Analysis System using smolagents

A comprehensive multi-agent system for data analysis powered by smolagents and Hugging Face models.
This system orchestrates specialized AI agents to perform end-to-end data science workflows.

Architecture:
- Manager Agent: Orchestrates the overall workflow
- Data Reader Agent: Analyzes datasets and provides structure information
- Data Manipulation Agent: Handles data preprocessing and transformations
- Data Operations Agent: Performs mathematical operations and aggregations
- ML Prediction Agent: Trains and evaluates machine learning models

Author: Multi-Agent System
Framework: smolagents (Hugging Face)
"""

import os
import json
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Union
from dotenv import load_dotenv

# Machine Learning imports
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.svm import SVC, SVR
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score, roc_auc_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_classif
import joblib

# Statistical imports
from scipy.stats import pearsonr, spearmanr, ttest_ind, chi2_contingency
from scipy import stats

# Visualization imports
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns

# smolagents imports
from smolagents import CodeAgent, InferenceClientModel, tool, TransformersModel

# Load environment variables
load_dotenv()

# ============================================================================
# CONFIGURATION
# ============================================================================

CONFIG = {
    # "model_id": os.getenv("MODEL_NAME", "deepseek-ai/deepseek-coder-7b-instruct-v1.5"),
    "model_id": os.getenv("MODEL_NAME", "Qwen/Qwen2.5-Coder-14B-Instruct"),
    "max_new_tokens": int(os.getenv("MAX_TOKENS", "1024")),
    "temperature": float(os.getenv("TEMPERATURE", "0.1")),
    "default_dataset": os.getenv("DEFAULT_DATASET_PATH", "data/titanic.csv"),
    "huggingface_token": os.getenv("HUGGINGFACE_TOKEN"),
}

print(f"ðŸš€ Multi-Agent System Configuration:")
print(f"   Model: {CONFIG['model_id']}")
print(f"   Max Tokens: {CONFIG['max_new_tokens']}")
print(f"   Temperature: {CONFIG['temperature']}")
print(f"   Default Dataset: {CONFIG['default_dataset']}")


# ============================================================================
# DATA READING TOOLS
# ============================================================================

@tool
def read_csv_file(file_path: str) -> str:
    """
    Read a CSV file and return basic information about the dataset.

    Args:
        file_path: Path to the CSV file

    Returns:
        String containing dataset information
    """
    try:
        df = pd.read_csv(file_path)

        info = {
            "status": "success",
            "shape": df.shape,
            "columns": list(df.columns),
            "memory_usage": f"{df.memory_usage(deep=True).sum() / 1024:.2f} KB"
        }

        return f"CSV file loaded successfully. Shape: {info['shape']}, Columns: {info['columns']}"

    except Exception as e:
        return f"Error reading CSV file: {str(e)}"


@tool
def read_json_file(file_path: str) -> str:
    """
    Read a JSON file and return basic information about the data.

    Args:
        file_path: Path to the JSON file

    Returns:
        String containing data information
    """
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)

        if isinstance(data, list):
            df = pd.DataFrame(data)
            return f"JSON file loaded successfully. Shape: {df.shape}, Columns: {list(df.columns)}"
        elif isinstance(data, dict):
            return f"JSON object loaded with keys: {list(data.keys())}"
        else:
            return f"JSON data loaded. Type: {type(data)}"

    except Exception as e:
        return f"Error reading JSON file: {str(e)}"


@tool
def get_column_info(file_path: str, column_name: Optional[str] = None) -> str:
    """
    Get detailed information about columns in the dataset.

    Args:
        file_path: Path to the data file
        column_name: Specific column to analyze (optional)

    Returns:
        String containing column information
    """
    try:
        df = pd.read_csv(file_path)

        if column_name:
            if column_name not in df.columns:
                return f"Column '{column_name}' not found in dataset"

            col = df[column_name]
            info = {
                "column": column_name,
                "data_type": str(col.dtype),
                "non_null_count": col.count(),
                "null_count": col.isnull().sum(),
                "unique_values": col.nunique(),
            }

            if col.dtype in ['int64', 'float64']:
                info.update({
                    "mean": col.mean(),
                    "std": col.std(),
                    "min": col.min(),
                    "max": col.max(),
                    "median": col.median()
                })
            elif col.dtype == 'object':
                info["sample_values"] = col.dropna().unique()[:10].tolist()

            return f"Column '{column_name}' info: {info}"
        else:
            # Return info for all columns
            column_info = []
            for col in df.columns:
                col_data = df[col]
                info = f"{col}: {col_data.dtype}, Non-null: {col_data.count()}/{len(df)}"
                column_info.append(info)

            return f"Dataset columns info:\n" + "\n".join(column_info)

    except Exception as e:
        return f"Error getting column info: {str(e)}"


@tool
def get_data_summary(file_path: str) -> str:
    """
    Get statistical summary of the dataset.

    Args:
        file_path: Path to the data file

    Returns:
        String containing statistical summary
    """
    try:
        df = pd.read_csv(file_path)

        # Get basic info
        info = {
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "memory_usage": f"{df.memory_usage(deep=True).sum() / 1024:.2f} KB"
        }

        # Get data types summary
        dtype_counts = df.dtypes.value_counts().to_dict()
        info["data_types"] = {str(k): v for k, v in dtype_counts.items()}

        # Get missing values summary
        missing_counts = df.isnull().sum()
        missing_info = missing_counts[missing_counts > 0].to_dict()
        info["missing_values"] = missing_info

        # Get numeric summary
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            info["numeric_summary"] = df[numeric_cols].describe().to_string()

        return f"Data Summary: {info}"

    except Exception as e:
        return f"Error getting data summary: {str(e)}"


@tool
def preview_data(file_path: str, num_rows: int = 5) -> str:
    """
    Preview the first few rows of the dataset.

    Args:
        file_path: Path to the data file
        num_rows: Number of rows to preview (default: 5)

    Returns:
        String containing preview data
    """
    try:
        df = pd.read_csv(file_path)

        preview = df.head(num_rows).to_string()

        return f"Data Preview (first {num_rows} rows):\n{preview}"

    except Exception as e:
        return f"Error previewing data: {str(e)}"


# ============================================================================
# DATA MANIPULATION TOOLS
# ============================================================================

@tool
def handle_missing_values(file_path: str, column_name: str, method: str, fill_value: Optional[Union[str, float, int]] = None) -> str:
    """
    Handle missing values in a column using various methods.

    Args:
        file_path: Path to the data file
        column_name: Name of the column with missing values
        method: Method to handle missing values (drop, mean, median, mode, forward_fill, backward_fill, constant)
        fill_value: Value to use for 'constant' method

    Returns:
        String describing the missing value handling performed
    """
    try:
        df = pd.read_csv(file_path)

        if column_name not in df.columns:
            return f"Column '{column_name}' not found in dataset"

        missing_count_before = df[column_name].isnull().sum()

        if missing_count_before == 0:
            return f"No missing values found in column '{column_name}'"

        if method == "drop":
            df = df.dropna(subset=[column_name])
        elif method == "mean":
            if df[column_name].dtype in ['int64', 'float64']:
                fill_val = df[column_name].mean()
                df[column_name] = df[column_name].fillna(fill_val)
            else:
                return f"Mean method not applicable for non-numeric column '{column_name}'"
        elif method == "median":
            if df[column_name].dtype in ['int64', 'float64']:
                fill_val = df[column_name].median()
                df[column_name] = df[column_name].fillna(fill_val)
            else:
                return f"Median method not applicable for non-numeric column '{column_name}'"
        elif method == "mode":
            mode_val = df[column_name].mode()
            if len(mode_val) > 0:
                df[column_name] = df[column_name].fillna(mode_val[0])
            else:
                return f"No mode found for column '{column_name}'"
        elif method == "forward_fill":
            df[column_name] = df[column_name].fillna(method='ffill')
        elif method == "backward_fill":
            df[column_name] = df[column_name].fillna(method='bfill')
        elif method == "constant" and fill_value is not None:
            df[column_name] = df[column_name].fillna(fill_value)
        else:
            return f"Unknown method '{method}' or missing fill_value for constant method"

        missing_count_after = df[column_name].isnull().sum()

        # Save the modified dataframe
        output_path = file_path.replace('.csv', '_missing_handled.csv')
        df.to_csv(output_path, index=False)

        return f"Handled {missing_count_before - missing_count_after} missing values in column '{column_name}' using method '{method}'. Remaining missing: {missing_count_after}. Saved to: {output_path}"

    except Exception as e:
        return f"Error handling missing values: {str(e)}"


@tool
def create_dummy_variables(file_path: str, column_name: str, prefix: Optional[str] = None) -> str:
    """
    Create dummy variables for a categorical column.

    Args:
        file_path: Path to the data file
        column_name: Name of the categorical column
        prefix: Prefix for dummy variable names (optional)

    Returns:
        String describing the dummy variables created
    """
    try:
        df = pd.read_csv(file_path)

        if column_name not in df.columns:
            return f"Column '{column_name}' not found in dataset"

        # Create dummy variables
        dummies = pd.get_dummies(df[column_name], prefix=prefix or column_name)

        # Add dummy variables to dataframe
        df_with_dummies = pd.concat([df, dummies], axis=1)

        # Save the modified dataframe
        output_path = file_path.replace('.csv', '_with_dummies.csv')
        df_with_dummies.to_csv(output_path, index=False)

        return f"Created {len(dummies.columns)} dummy variables for '{column_name}': {list(dummies.columns)}. Saved to: {output_path}"

    except Exception as e:
        return f"Error creating dummy variables: {str(e)}"


@tool
def modify_column_values(file_path: str, column_name: str, operation: str, value: Optional[Union[str, float, int]] = None) -> str:
    """
    Modify values in a column using various operations.

    Args:
        file_path: Path to the data file
        column_name: Name of the column to modify
        operation: Type of operation (multiply, add, subtract, divide, replace, normalize, standardize)
        value: Value to use in the operation

    Returns:
        String describing the modification performed
    """
    try:
        df = pd.read_csv(file_path)

        if column_name not in df.columns:
            return f"Column '{column_name}' not found in dataset"

        original_column = df[column_name].copy()

        if operation == "multiply" and value is not None:
            df[column_name] = df[column_name] * float(value)
        elif operation == "add" and value is not None:
            df[column_name] = df[column_name] + float(value)
        elif operation == "subtract" and value is not None:
            df[column_name] = df[column_name] - float(value)
        elif operation == "divide" and value is not None:
            df[column_name] = df[column_name] / float(value)
        elif operation == "replace" and value is not None:
            # For replace, value should be in format "old_value,new_value"
            if isinstance(value, str) and "," in value:
                old_val, new_val = value.split(",", 1)
                df[column_name] = df[column_name].replace(old_val.strip(), new_val.strip())
            else:
                return "For replace operation, value should be in format 'old_value,new_value'"
        elif operation == "normalize":
            # Normalize to 0-1 range
            min_val, max_val = df[column_name].min(), df[column_name].max()
            df[column_name] = (df[column_name] - min_val) / (max_val - min_val)
        elif operation == "standardize":
            # Standardize to mean=0, std=1
            mean_val, std_val = df[column_name].mean(), df[column_name].std()
            df[column_name] = (df[column_name] - mean_val) / std_val
        else:
            return f"Unknown operation '{operation}' or missing value parameter"

        # Save the modified dataframe
        output_path = file_path.replace('.csv', '_modified.csv')
        df.to_csv(output_path, index=False)

        # Calculate changes
        changed_count = (df[column_name] != original_column).sum()

        return f"Modified {changed_count} values in column '{column_name}' using operation '{operation}'. Saved to: {output_path}"

    except Exception as e:
        return f"Error modifying column values: {str(e)}"


@tool
def convert_data_types(file_path: str, column_name: str, target_type: str) -> str:
    """
    Convert data type of a column.

    Args:
        file_path: Path to the data file
        column_name: Name of the column to convert
        target_type: Target data type (int, float, string, category, datetime)

    Returns:
        String describing the data type conversion performed
    """
    try:
        df = pd.read_csv(file_path)

        if column_name not in df.columns:
            return f"Column '{column_name}' not found in dataset"

        original_dtype = df[column_name].dtype

        if target_type == "int":
            df[column_name] = pd.to_numeric(df[column_name], errors='coerce').astype('Int64')
        elif target_type == "float":
            df[column_name] = pd.to_numeric(df[column_name], errors='coerce')
        elif target_type == "string":
            df[column_name] = df[column_name].astype(str)
        elif target_type == "category":
            df[column_name] = df[column_name].astype('category')
        elif target_type == "datetime":
            df[column_name] = pd.to_datetime(df[column_name], errors='coerce')
        else:
            return f"Unknown target type '{target_type}'. Supported types: int, float, string, category, datetime"

        new_dtype = df[column_name].dtype

        # Save the modified dataframe
        output_path = file_path.replace('.csv', '_type_converted.csv')
        df.to_csv(output_path, index=False)

        return f"Converted column '{column_name}' from {original_dtype} to {new_dtype}. Saved to: {output_path}"

    except Exception as e:
        return f"Error converting data type: {str(e)}"


# ============================================================================
# DATA OPERATIONS TOOLS
# ============================================================================

@tool
def filter_data(file_path: str, column_name: str, condition: str, value: Union[str, float, int]) -> str:
    """
    Filter data based on a condition.

    Args:
        file_path: Path to the data file
        column_name: Name of the column to filter on
        condition: Condition type (equals, not_equals, greater_than, less_than, greater_equal, less_equal, contains)
        value: Value to filter by

    Returns:
        String describing the filtering performed
    """
    try:
        df = pd.read_csv(file_path)

        if column_name not in df.columns:
            return f"Column '{column_name}' not found in dataset"

        original_rows = len(df)

        if condition == "equals":
            filtered_df = df[df[column_name] == value]
        elif condition == "not_equals":
            filtered_df = df[df[column_name] != value]
        elif condition == "greater_than":
            filtered_df = df[df[column_name] > float(value)]
        elif condition == "less_than":
            filtered_df = df[df[column_name] < float(value)]
        elif condition == "greater_equal":
            filtered_df = df[df[column_name] >= float(value)]
        elif condition == "less_equal":
            filtered_df = df[df[column_name] <= float(value)]
        elif condition == "contains":
            filtered_df = df[df[column_name].astype(str).str.contains(str(value), na=False)]
        else:
            return f"Unknown condition '{condition}'"

        filtered_rows = len(filtered_df)

        # Save the filtered dataframe
        output_path = file_path.replace('.csv', '_filtered.csv')
        filtered_df.to_csv(output_path, index=False)

        return f"Filtered data: {original_rows} -> {filtered_rows} rows using condition '{column_name} {condition} {value}'. Saved to: {output_path}"

    except Exception as e:
        return f"Error filtering data: {str(e)}"


@tool
def perform_math_operations(file_path: str, operation: str, column1: str, column2: Optional[str] = None, value: Optional[float] = None) -> str:
    """
    Perform mathematical operations on columns.

    Args:
        file_path: Path to the data file
        operation: Type of operation (add, subtract, multiply, divide, power, square, sqrt, log, abs)
        column1: First column name
        column2: Second column name (for two-column operations)
        value: Numeric value (for column-value operations)

    Returns:
        String describing the mathematical operation performed
    """
    try:
        df = pd.read_csv(file_path)

        if column1 not in df.columns:
            return f"Column '{column1}' not found in dataset"

        if column2 and column2 not in df.columns:
            return f"Column '{column2}' not found in dataset"

        # Create new column name for result
        if column2:
            new_col_name = f"{column1}_{operation}_{column2}"
        elif value is not None:
            new_col_name = f"{column1}_{operation}_{value}"
        else:
            new_col_name = f"{column1}_{operation}"

        # Perform operations
        if operation == "add":
            if column2:
                df[new_col_name] = df[column1] + df[column2]
            elif value is not None:
                df[new_col_name] = df[column1] + value
        elif operation == "subtract":
            if column2:
                df[new_col_name] = df[column1] - df[column2]
            elif value is not None:
                df[new_col_name] = df[column1] - value
        elif operation == "multiply":
            if column2:
                df[new_col_name] = df[column1] * df[column2]
            elif value is not None:
                df[new_col_name] = df[column1] * value
        elif operation == "divide":
            if column2:
                df[new_col_name] = df[column1] / df[column2]
            elif value is not None:
                df[new_col_name] = df[column1] / value
        elif operation == "power":
            if value is not None:
                df[new_col_name] = df[column1] ** value
            else:
                return "Power operation requires a value parameter"
        elif operation == "square":
            df[new_col_name] = df[column1] ** 2
        elif operation == "sqrt":
            df[new_col_name] = np.sqrt(df[column1])
        elif operation == "log":
            df[new_col_name] = np.log(df[column1])
        elif operation == "abs":
            df[new_col_name] = np.abs(df[column1])
        else:
            return f"Unknown operation '{operation}'"

        # Save the modified dataframe
        output_path = file_path.replace('.csv', '_math_ops.csv')
        df.to_csv(output_path, index=False)

        return f"Created new column '{new_col_name}' using operation '{operation}'. Saved to: {output_path}"

    except Exception as e:
        return f"Error performing math operation: {str(e)}"


@tool
def aggregate_data(file_path: str, group_by_columns: str, agg_column: str, agg_function: str) -> str:
    """
    Aggregate data by grouping columns.

    Args:
        file_path: Path to the data file
        group_by_columns: Comma-separated list of columns to group by
        agg_column: Column to aggregate
        agg_function: Aggregation function (mean, sum, count, min, max, std, median)

    Returns:
        String describing the aggregation performed
    """
    try:
        df = pd.read_csv(file_path)

        # Parse group by columns
        group_cols = [col.strip() for col in group_by_columns.split(',')]

        # Check if columns exist
        for col in group_cols + [agg_column]:
            if col not in df.columns:
                return f"Column '{col}' not found in dataset"

        # Perform aggregation
        if agg_function == "mean":
            result_df = df.groupby(group_cols)[agg_column].mean().reset_index()
        elif agg_function == "sum":
            result_df = df.groupby(group_cols)[agg_column].sum().reset_index()
        elif agg_function == "count":
            result_df = df.groupby(group_cols)[agg_column].count().reset_index()
        elif agg_function == "min":
            result_df = df.groupby(group_cols)[agg_column].min().reset_index()
        elif agg_function == "max":
            result_df = df.groupby(group_cols)[agg_column].max().reset_index()
        elif agg_function == "std":
            result_df = df.groupby(group_cols)[agg_column].std().reset_index()
        elif agg_function == "median":
            result_df = df.groupby(group_cols)[agg_column].median().reset_index()
        else:
            return f"Unknown aggregation function '{agg_function}'"

        # Rename the aggregated column
        result_df = result_df.rename(columns={agg_column: f"{agg_column}_{agg_function}"})

        # Save the aggregated dataframe
        output_path = file_path.replace('.csv', '_aggregated.csv')
        result_df.to_csv(output_path, index=False)

        return f"Aggregated data by {group_cols} using {agg_function} on '{agg_column}'. Result shape: {result_df.shape}. Result:\n{result_df.to_string()}\nSaved to: {output_path}"

    except Exception as e:
        return f"Error aggregating data: {str(e)}"


@tool
def string_operations(file_path: str, column_name: str, operation: str, parameter: Optional[str] = None) -> str:
    """
    Perform string operations on a text column.

    Args:
        file_path: Path to the data file
        column_name: Name of the text column
        operation: Type of operation (upper, lower, title, length, contains_count, split, replace, strip)
        parameter: Additional parameter for some operations

    Returns:
        String describing the string operation performed
    """
    try:
        df = pd.read_csv(file_path)

        if column_name not in df.columns:
            return f"Column '{column_name}' not found in dataset"

        # Convert to string type
        df[column_name] = df[column_name].astype(str)

        if operation == "upper":
            new_col_name = f"{column_name}_upper"
            df[new_col_name] = df[column_name].str.upper()
        elif operation == "lower":
            new_col_name = f"{column_name}_lower"
            df[new_col_name] = df[column_name].str.lower()
        elif operation == "title":
            new_col_name = f"{column_name}_title"
            df[new_col_name] = df[column_name].str.title()
        elif operation == "length":
            new_col_name = f"{column_name}_length"
            df[new_col_name] = df[column_name].str.len()
        elif operation == "contains_count" and parameter:
            new_col_name = f"{column_name}_contains_{parameter}"
            df[new_col_name] = df[column_name].str.count(parameter)
        elif operation == "split" and parameter:
            # Split and create new columns
            split_cols = df[column_name].str.split(parameter, expand=True)
            for i, col in enumerate(split_cols.columns):
                df[f"{column_name}_part_{i+1}"] = split_cols[col]
            new_col_name = f"split into {len(split_cols.columns)} columns"
        elif operation == "replace" and parameter:
            # Parameter should be "old,new"
            if "," in parameter:
                old, new = parameter.split(",", 1)
                new_col_name = f"{column_name}_replaced"
                df[new_col_name] = df[column_name].str.replace(old.strip(), new.strip())
            else:
                return "Replace operation requires parameter in format 'old,new'"
        elif operation == "strip":
            new_col_name = f"{column_name}_stripped"
            df[new_col_name] = df[column_name].str.strip()
        else:
            return f"Unknown string operation '{operation}'"

        # Save the modified dataframe
        output_path = file_path.replace('.csv', '_string_ops.csv')
        df.to_csv(output_path, index=False)

        return f"Applied string operation '{operation}' to column '{column_name}'. Result: {new_col_name}. Saved to: {output_path}"

    except Exception as e:
        return f"Error performing string operation: {str(e)}"


# ============================================================================
# MACHINE LEARNING TOOLS
# ============================================================================

@tool
def train_regression_model(file_path: str, target_column: str, feature_columns: str, model_type: str = "linear") -> str:
    """
    Train a regression model.

    Args:
        file_path: Path to the data file
        target_column: Name of the target variable column
        feature_columns: Comma-separated list of feature columns
        model_type: Type of regression (linear, random_forest)

    Returns:
        String describing the model training results
    """
    try:
        df = pd.read_csv(file_path)

        # Parse feature columns
        features = [col.strip() for col in feature_columns.split(',')]

        # Check if columns exist
        for col in features + [target_column]:
            if col not in df.columns:
                return f"Column '{col}' not found in dataset"

        # Prepare data
        X = df[features]
        y = df[target_column]

        # Handle missing values
        X = X.fillna(X.mean())
        y = y.fillna(y.mean())

        # Handle categorical variables in features
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train model
        if model_type == "linear":
            model = LinearRegression()
        elif model_type == "random_forest":
            model = RandomForestRegressor(n_estimators=100, random_state=42)
        else:
            return f"Unknown regression model type '{model_type}'"

        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate metrics
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mse)

        # Cross-validation
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')

        # Save model
        model_path = file_path.replace('.csv', f'_{model_type}_regression_model.joblib')
        joblib.dump(model, model_path)

        results = {
            "model_type": f"{model_type} regression",
            "features": features,
            "target": target_column,
            "train_size": len(X_train),
            "test_size": len(X_test),
            "mse": round(mse, 4),
            "rmse": round(rmse, 4),
            "r2_score": round(r2, 4),
            "cv_r2_mean": round(cv_scores.mean(), 4),
            "cv_r2_std": round(cv_scores.std(), 4),
            "model_saved": model_path
        }

        return f"Regression model trained: {results}"

    except Exception as e:
        return f"Error training regression model: {str(e)}"


# @tool
# def train_svm_model(file_path: str, target_column: str, feature_columns: str, task_type: str = "classification") -> str:
#     """
#     Train a Support Vector Machine model.

#     Args:
#         file_path: Path to the data file
#         target_column: Name of the target variable column
#         feature_columns: Comma-separated list of feature columns
#         task_type: Type of task (classification, regression)

#     Returns:
#         String describing the model training results
#     """
#     try:
#         df = pd.read_csv(file_path)

#         # Parse feature columns
#         features = [col.strip() for col in feature_columns.split(',')]

#         # Check if columns exist
#         for col in features + [target_column]:
#             if col not in df.columns:
#                 return f"Column '{col}' not found in dataset"

#         # Prepare data
#         X = df[features]
#         y = df[target_column]

#         # Handle missing values
#         X = X.fillna(X.mean())
#         if task_type == "classification":
#             y = y.fillna(y.mode()[0])
#         else:
#             y = y.fillna(y.mean())

#         # Handle categorical variables
#         for col in X.columns:
#             if X[col].dtype == 'object':
#                 le = LabelEncoder()
#                 X[col] = le.fit_transform(X[col].astype(str))

#         if task_type == "classification" and y.dtype == 'object':
#             le_target = LabelEncoder()
#             y = le_target.fit_transform(y.astype(str))

#         # Split data
#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#         # Train model
#         if task_type == "classification":
#             model = SVC(kernel='rbf', random_state=42)
#             scoring_metric = 'accuracy'
#         else:
#             model = SVR(kernel='rbf')
#             scoring_metric = 'r2'

#         model.fit(X_train, y_train)

#         # Make predictions
#         y_pred = model.predict(X_test)

#         # Calculate metrics
#         if task_type == "classification":
#             accuracy = accuracy_score(y_test, y_pred)
#             precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
#             recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
#             f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

#             metrics = {
#                 "accuracy": round(accuracy, 4),
#                 "precision": round(precision, 4),
#                 "recall": round(recall, 4),
#                 "f1_score": round(f1, 4)
#             }
#         else:
#             mse = mean_squared_error(y_test, y_pred)
#             r2 = r2_score(y_test, y_pred)

#             metrics = {
#                 "mse": round(mse, 4),
#                 "r2_score": round(r2, 4)
#             }

#         # Cross-validation
#         cv_scores = cross_val_score(model, X, y, cv=5, scoring=scoring_metric)

#         # Save model
#         model_path = file_path.replace('.csv', f'_svm_{task_type}_model.joblib')
#         joblib.dump(model, model_path)

#         results = {
#             "model_type": f"SVM {task_type}",
#             "features": features,
#             "target": target_column,
#             "train_size": len(X_train),
#             "test_size": len(X_test),
#             metrics,
#             "cv_score_mean": round(cv_scores.mean(), 4),
#             "cv_score_std": round(cv_scores.std(), 4),
#             "model_saved": model_path
#         }

#         return f"SVM model trained: {results}"

#     except Exception as e:
#         return f"Error training SVM model: {str(e)}"


# @tool
# def train_random_forest_model(file_path: str, target_column: str, feature_columns: str, task_type: str = "classification") -> str:
#     """
#     Train a Random Forest model.

#     Args:
#         file_path: Path to the data file
#         target_column: Name of the target variable column
#         feature_columns: Comma-separated list of feature columns
#         task_type: Type of task (classification, regression)

#     Returns:
#         String describing the model training results including feature importance
#     """
#     try:
#         df = pd.read_csv(file_path)

#         # Parse feature columns
#         features = [col.strip() for col in feature_columns.split(',')]

#         # Check if columns exist
#         for col in features + [target_column]:
#             if col not in df.columns:
#                 return f"Column '{col}' not found in dataset"

#         # Prepare data
#         X = df[features]
#         y = df[target_column]

#         # Handle missing values
#         X = X.fillna(X.mean())
#         if task_type == "classification":
#             y = y.fillna(y.mode()[0])
#         else:
#             y = y.fillna(y.mean())

#         # Handle categorical variables
#         for col in X.columns:
#             if X[col].dtype == 'object':
#                 le = LabelEncoder()
#                 X[col] = le.fit_transform(X[col].astype(str))

#         if task_type == "classification" and y.dtype == 'object':
#             le_target = LabelEncoder()
#             y = le_target.fit_transform(y.astype(str))

#         # Split data
#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#         # Train model
#         if task_type == "classification":
#             model = RandomForestClassifier(n_estimators=100, random_state=42)
#             scoring_metric = 'accuracy'
#         else:
#             model = RandomForestRegressor(n_estimators=100, random_state=42)
#             scoring_metric = 'r2'

#         model.fit(X_train, y_train)

#         # Make predictions
#         y_pred = model.predict(X_test)

#         # Calculate metrics
#         if task_type == "classification":
#             accuracy = accuracy_score(y_test, y_pred)
#             precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
#             recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
#             f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

#             metrics = {
#                 "accuracy": round(accuracy, 4),
#                 "precision": round(precision, 4),
#                 "recall": round(recall, 4),
#                 "f1_score": round(f1, 4)
#             }
#         else:
#             mse = mean_squared_error(y_test, y_pred)
#             r2 = r2_score(y_test, y_pred)

#             metrics = {
#                 "mse": round(mse, 4),
#                 "r2_score": round(r2, 4)
#             }

#         # Feature importance
#         feature_importance = dict(zip(features, [round(imp, 4) for imp in model.feature_importances_]))

#         # Cross-validation
#         cv_scores = cross_val_score(model, X, y, cv=5, scoring=scoring_metric)

#         # Save model
#         model_path = file_path.replace('.csv', f'_rf_{task_type}_model.joblib')
#         joblib.dump(model, model_path)

#         results = {
#             "model_type": f"Random Forest {task_type}",
#             "features": features,
#             "target": target_column,
#             "train_size": len(X_train),
#             "test_size": len(X_test),
#             metrics,
#             "cv_score_mean": round(cv_scores.mean(), 4),
#             "cv_score_std": round(cv_scores.std(), 4),
#             "feature_importance": feature_importance,
#             "model_saved": model_path
#         }

#         return f"Random Forest model trained: {results}"

#     except Exception as e:
#         return f"Error training Random Forest model: {str(e)}"


# @tool
# def train_knn_model(file_path: str, target_column: str, feature_columns: str, task_type: str = "classification", n_neighbors: int = 5) -> str:
#     """
#     Train a K-Nearest Neighbors model.

#     Args:
#         file_path: Path to the data file
#         target_column: Name of the target variable column
#         feature_columns: Comma-separated list of feature columns
#         task_type: Type of task (classification, regression)
#         n_neighbors: Number of neighbors to use

#     Returns:
#         String describing the model training results
#     """
#     try:
#         df = pd.read_csv(file_path)

#         # Parse feature columns
#         features = [col.strip() for col in feature_columns.split(',')]

#         # Check if columns exist
#         for col in features + [target_column]:
#             if col not in df.columns:
#                 return f"Column '{col}' not found in dataset"

#         # Prepare data
#         X = df[features]
#         y = df[target_column]

#         # Handle missing values
#         X = X.fillna(X.mean())
#         if task_type == "classification":
#             y = y.fillna(y.mode()[0])
#         else:
#             y = y.fillna(y.mean())

#         # Handle categorical variables
#         for col in X.columns:
#             if X[col].dtype == 'object':
#                 le = LabelEncoder()
#                 X[col] = le.fit_transform(X[col].astype(str))

#         if task_type == "classification" and y.dtype == 'object':
#             le_target = LabelEncoder()
#             y = le_target.fit_transform(y.astype(str))

#         # Split data
#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#         # Train model
#         if task_type == "classification":
#             model = KNeighborsClassifier(n_neighbors=n_neighbors)
#             scoring_metric = 'accuracy'
#         else:
#             model = KNeighborsRegressor(n_neighbors=n_neighbors)
#             scoring_metric = 'r2'

#         model.fit(X_train, y_train)

#         # Make predictions
#         y_pred = model.predict(X_test)

#         # Calculate metrics
#         if task_type == "classification":
#             accuracy = accuracy_score(y_test, y_pred)
#             precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
#             recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
#             f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

#             metrics = {
#                 "accuracy": round(accuracy, 4),
#                 "precision": round(precision, 4),
#                 "recall": round(recall, 4),
#                 "f1_score": round(f1, 4)
#             }
#         else:
#             mse = mean_squared_error(y_test, y_pred)
#             r2 = r2_score(y_test, y_pred)

#             metrics = {
#                 "mse": round(mse, 4),
#                 "r2_score": round(r2, 4)
#             }

#         # Cross-validation
#         cv_scores = cross_val_score(model, X, y, cv=5, scoring=scoring_metric)

#         # Save model
#         model_path = file_path.replace('.csv', f'_knn_{task_type}_model.joblib')
#         joblib.dump(model, model_path)

#         results = {
#             "model_type": f"KNN {task_type}",
#             "n_neighbors": n_neighbors,
#             "features": features,
#             "target": target_column,
#             "train_size": len(X_train),
#             "test_size": len(X_test),
#             metrics,
#             "cv_score_mean": round(cv_scores.mean(), 4),
#             "cv_score_std": round(cv_scores.std(), 4),
#             "model_saved": model_path
#         }

#         return f"KNN model trained: {results}"

#     except Exception as e:
#         return f"Error training KNN model: {str(e)}"


# @tool
# def evaluate_model(model_path: str, test_data_path: str, target_column: str, feature_columns: str) -> str:
#     """
#     Evaluate a trained model on new test data.

#     Args:
#         model_path: Path to the saved model file
#         test_data_path: Path to the test data file
#         target_column: Name of the target variable column
#         feature_columns: Comma-separated list of feature columns

#     Returns:
#         String describing the model evaluation results
#     """
#     try:
#         # Load the model
#         model = joblib.load(model_path)

#         # Load test data
#         df = pd.read_csv(test_data_path)

#         # Parse feature columns
#         features = [col.strip() for col in feature_columns.split(',')]

#         # Check if columns exist
#         for col in features + [target_column]:
#             if col not in df.columns:
#                 return f"Column '{col}' not found in test dataset"

#         # Prepare test data
#         X = df[features]
#         y = df[target_column]

#         # Handle missing values
#         X = X.fillna(X.mean())

#         # Handle categorical variables (same as training)
#         for col in X.columns:
#             if X[col].dtype == 'object':
#                 le = LabelEncoder()
#                 X[col] = le.fit_transform(X[col].astype(str))

#         # Make predictions
#         y_pred = model.predict(X)

#         # Determine if this is classification or regression based on model type
#         model_name = str(type(model).__name__)
#         is_classification = any(clf in model_name for clf in ['Classifier', 'SVC'])

#         # Calculate appropriate metrics
#         if is_classification:
#             accuracy = accuracy_score(y, y_pred)
#             precision = precision_score(y, y_pred, average='weighted', zero_division=0)
#             recall = recall_score(y, y_pred, average='weighted', zero_division=0)
#             f1 = f1_score(y, y_pred, average='weighted', zero_division=0)

#             results = {
#                 "model_type": model_name,
#                 "task": "classification",
#                 "test_samples": len(X),
#                 "accuracy": round(accuracy, 4),
#                 "precision": round(precision, 4),
#                 "recall": round(recall, 4),
#                 "f1_score": round(f1, 4)
#             }
#         else:
#             mse = mean_squared_error(y, y_pred)
#             r2 = r2_score(y, y_pred)
#             rmse = np.sqrt(mse)

#             results = {
#                 "model_type": model_name,
#                 "task": "regression",
#                 "test_samples": len(X),
#                 "mse": round(mse, 4),
#                 "rmse": round(rmse, 4),
#                 "r2_score": round(r2, 4)
#             }

#         return f"Model evaluation results: {results}"

#     except Exception as e:
#         return f"Error evaluating model: {str(e)}"
    
# ============================================================================
# DATA READING TOOLS - Simplified and Specific
# ============================================================================

@tool
def load_dataset(file_path: str) -> str:
    """
    Load a dataset and return basic shape information.
    
    Args:
        file_path: Path to the CSV file
        
    Returns:
        String with dataset shape (rows, columns)
    """
    try:
        df = pd.read_csv(file_path)
        return f"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns"
    except Exception as e:
        return f"Error loading dataset: {str(e)}"


@tool
def get_column_names(file_path: str) -> str:
    """
    Get list of all column names in the dataset.
    
    Args:
        file_path: Path to the CSV file
        
    Returns:
        Comma-separated list of column names
    """
    try:
        df = pd.read_csv(file_path)
        return f"Columns: {', '.join(df.columns.tolist())}"
    except Exception as e:
        return f"Error getting columns: {str(e)}"


@tool
def get_data_types(file_path: str) -> str:
    """
    Get data types of all columns.
    
    Args:
        file_path: Path to the CSV file
        
    Returns:
        String with column names and their data types
    """
    try:
        df = pd.read_csv(file_path)
        dtypes = df.dtypes.to_dict()
        result = []
        for col, dtype in dtypes.items():
            result.append(f"{col}: {dtype}")
        return "Data types:\n" + "\n".join(result)
    except Exception as e:
        return f"Error getting data types: {str(e)}"


@tool
def get_null_counts(file_path: str) -> str:
    """
    Count missing values in each column.
    
    Args:
        file_path: Path to the CSV file
        
    Returns:
        String with missing value counts per column
    """
    try:
        df = pd.read_csv(file_path)
        null_counts = df.isnull().sum()
        result = []
        for col, count in null_counts.items():
            if count > 0:
                pct = (count / len(df)) * 100
                result.append(f"{col}: {count} ({pct:.1f}%)")
        
        if result:
            return "Missing values:\n" + "\n".join(result)
        else:
            return "No missing values found"
    except Exception as e:
        return f"Error counting nulls: {str(e)}"


@tool
def get_unique_values(file_path: str, column_name: str) -> str:
    """
    Get unique values in a specific column.
    
    Args:
        file_path: Path to the CSV file
        column_name: Name of the column
        
    Returns:
        String with unique values and their count
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        unique_vals = df[column_name].dropna().unique()
        n_unique = len(unique_vals)
        
        if n_unique <= 20:
            return f"Column '{column_name}' has {n_unique} unique values: {', '.join(map(str, unique_vals))}"
        else:
            sample = unique_vals[:10]
            return f"Column '{column_name}' has {n_unique} unique values. First 10: {', '.join(map(str, sample))}"
    except Exception as e:
        return f"Error getting unique values: {str(e)}"


@tool
def get_numeric_summary(file_path: str, column_name: str) -> str:
    """
    Get statistical summary for a numeric column.
    
    Args:
        file_path: Path to the CSV file
        column_name: Name of the numeric column
        
    Returns:
        String with mean, median, std, min, max, quartiles
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        col = df[column_name].dropna()
        
        stats = {
            "count": len(col),
            "mean": col.mean(),
            "median": col.median(),
            "std": col.std(),
            "min": col.min(),
            "25%": col.quantile(0.25),
            "50%": col.quantile(0.50),
            "75%": col.quantile(0.75),
            "max": col.max()
        }
        
        result = f"Statistics for '{column_name}':\n"
        for key, val in stats.items():
            result += f"  {key}: {val:.2f}\n"
        
        return result
    except Exception as e:
        return f"Error getting summary: {str(e)}"


@tool
def get_first_rows(file_path: str, n_rows: int = 5) -> str:
    """
    Get first N rows of the dataset.
    
    Args:
        file_path: Path to the CSV file
        n_rows: Number of rows to return
        
    Returns:
        String representation of first N rows
    """
    try:
        df = pd.read_csv(file_path)
        preview = df.head(n_rows).to_string()
        return f"First {n_rows} rows:\n{preview}"
    except Exception as e:
        return f"Error getting rows: {str(e)}"


# ============================================================================
# DATA MANIPULATION TOOLS - More Specific
# ============================================================================

@tool
def drop_column(file_path: str, column_name: str) -> str:
    """
    Drop a specific column from the dataset.
    
    Args:
        file_path: Path to the CSV file
        column_name: Name of column to drop
        
    Returns:
        String confirming column drop and new file path
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        df = df.drop(columns=[column_name])
        output_path = file_path.replace('.csv', '_dropped.csv')
        df.to_csv(output_path, index=False)
        
        return f"Dropped column '{column_name}'. New shape: {df.shape}. Saved to: {output_path}"
    except Exception as e:
        return f"Error dropping column: {str(e)}"


@tool
def drop_null_rows(file_path: str, column_name: Optional[str] = None) -> str:
    """
    Drop rows with missing values.
    
    Args:
        file_path: Path to the CSV file
        column_name: Specific column to check for nulls (optional, if None drops any row with null)
        
    Returns:
        String confirming rows dropped
    """
    try:
        df = pd.read_csv(file_path)
        original_len = len(df)
        
        if column_name:
            if column_name not in df.columns:
                return f"Column '{column_name}' not found"
            df = df.dropna(subset=[column_name])
        else:
            df = df.dropna()
        
        dropped = original_len - len(df)
        output_path = file_path.replace('.csv', '_no_nulls.csv')
        df.to_csv(output_path, index=False)
        
        return f"Dropped {dropped} rows with nulls. New shape: {df.shape}. Saved to: {output_path}"
    except Exception as e:
        return f"Error dropping null rows: {str(e)}"


@tool
def fill_numeric_nulls(file_path: str, column_name: str, method: str = "mean") -> str:
    """
    Fill missing values in a numeric column.
    
    Args:
        file_path: Path to the CSV file
        column_name: Name of the numeric column
        method: Fill method - 'mean', 'median', 'zero', or numeric value
        
    Returns:
        String confirming fill operation
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        null_count = df[column_name].isnull().sum()
        if null_count == 0:
            return f"No missing values in '{column_name}'"
        
        if method == "mean":
            fill_value = df[column_name].mean()
        elif method == "median":
            fill_value = df[column_name].median()
        elif method == "zero":
            fill_value = 0
        else:
            try:
                fill_value = float(method)
            except:
                return f"Invalid method: {method}"
        
        df[column_name] = df[column_name].fillna(fill_value)
        
        output_path = file_path.replace('.csv', '_filled.csv')
        df.to_csv(output_path, index=False)
        
        return f"Filled {null_count} nulls in '{column_name}' with {method} ({fill_value:.2f}). Saved to: {output_path}"
    except Exception as e:
        return f"Error filling nulls: {str(e)}"


@tool
def fill_categorical_nulls(file_path: str, column_name: str, method: str = "mode") -> str:
    """
    Fill missing values in a categorical column.
    
    Args:
        file_path: Path to the CSV file
        column_name: Name of the categorical column
        method: Fill method - 'mode', 'unknown', or specific value
        
    Returns:
        String confirming fill operation
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        null_count = df[column_name].isnull().sum()
        if null_count == 0:
            return f"No missing values in '{column_name}'"
        
        if method == "mode":
            fill_value = df[column_name].mode()[0] if len(df[column_name].mode()) > 0 else "unknown"
        elif method == "unknown":
            fill_value = "unknown"
        else:
            fill_value = method
        
        df[column_name] = df[column_name].fillna(fill_value)
        
        output_path = file_path.replace('.csv', '_filled.csv')
        df.to_csv(output_path, index=False)
        
        return f"Filled {null_count} nulls in '{column_name}' with '{fill_value}'. Saved to: {output_path}"
    except Exception as e:
        return f"Error filling categorical nulls: {str(e)}"


@tool
def encode_categorical(file_path: str, column_name: str, encoding_type: str = "onehot") -> str:
    """
    Encode a categorical column.
    
    Args:
        file_path: Path to the CSV file
        column_name: Name of categorical column
        encoding_type: 'onehot' or 'label'
        
    Returns:
        String confirming encoding operation
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        if encoding_type == "onehot":
            dummies = pd.get_dummies(df[column_name], prefix=column_name, dtype=int)
            df = pd.concat([df, dummies], axis=1)
            df = df.drop(columns=[column_name])
            msg = f"One-hot encoded '{column_name}' into {len(dummies.columns)} columns"
        elif encoding_type == "label":
            le = LabelEncoder()
            df[f"{column_name}_encoded"] = le.fit_transform(df[column_name].fillna('missing'))
            mapping = dict(zip(le.classes_, le.transform(le.classes_)))
            msg = f"Label encoded '{column_name}'. Mapping: {mapping}"
        else:
            return f"Unknown encoding type: {encoding_type}"
        
        output_path = file_path.replace('.csv', '_encoded.csv')
        df.to_csv(output_path, index=False)
        
        return f"{msg}. Saved to: {output_path}"
    except Exception as e:
        return f"Error encoding: {str(e)}"


@tool
def create_new_feature(file_path: str, new_column: str, column1: str, operation: str, column2_or_value: Union[str, float]) -> str:
    """
    Create a new feature from existing columns.
    
    Args:
        file_path: Path to the CSV file
        new_column: Name for the new feature
        column1: First column name
        operation: Operation to perform (+, -, *, /, >, <, ==)
        column2_or_value: Second column name or numeric value
        
    Returns:
        String confirming feature creation
    """
    try:
        df = pd.read_csv(file_path)
        
        if column1 not in df.columns:
            return f"Column '{column1}' not found"
        
        # Check if column2_or_value is a column name or a value
        if isinstance(column2_or_value, str) and column2_or_value in df.columns:
            operand2 = df[column2_or_value]
        else:
            try:
                operand2 = float(column2_or_value)
            except:
                operand2 = column2_or_value
        
        operand1 = df[column1]
        
        if operation == '+':
            df[new_column] = operand1 + operand2
        elif operation == '-':
            df[new_column] = operand1 - operand2
        elif operation == '*':
            df[new_column] = operand1 * operand2
        elif operation == '/':
            df[new_column] = operand1 / operand2
        elif operation == '>':
            df[new_column] = (operand1 > operand2).astype(int)
        elif operation == '<':
            df[new_column] = (operand1 < operand2).astype(int)
        elif operation == '==':
            df[new_column] = (operand1 == operand2).astype(int)
        else:
            return f"Unknown operation: {operation}"
        
        output_path = file_path.replace('.csv', '_new_feature.csv')
        df.to_csv(output_path, index=False)
        
        return f"Created new feature '{new_column}' using {column1} {operation} {column2_or_value}. Saved to: {output_path}"
    except Exception as e:
        return f"Error creating feature: {str(e)}"


@tool
def normalize_column(file_path: str, column_name: str, method: str = "minmax") -> str:
    """
    Normalize a numeric column.
    
    Args:
        file_path: Path to the CSV file
        column_name: Name of column to normalize
        method: 'minmax' (0-1) or 'zscore' (standardization)
        
    Returns:
        String confirming normalization
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        col = df[column_name].values.reshape(-1, 1)
        
        if method == "minmax":
            scaler = MinMaxScaler()
            df[f"{column_name}_normalized"] = scaler.fit_transform(col)
            msg = f"Min-max normalized '{column_name}' to range [0, 1]"
        elif method == "zscore":
            scaler = StandardScaler()
            df[f"{column_name}_standardized"] = scaler.fit_transform(col)
            msg = f"Z-score standardized '{column_name}' (mean=0, std=1)"
        else:
            return f"Unknown method: {method}"
        
        output_path = file_path.replace('.csv', '_normalized.csv')
        df.to_csv(output_path, index=False)
        
        return f"{msg}. Saved to: {output_path}"
    except Exception as e:
        return f"Error normalizing: {str(e)}"


# ============================================================================
# DATA FILTERING AND SELECTION TOOLS
# ============================================================================

@tool
def filter_rows_numeric(file_path: str, column_name: str, operator: str, value: float) -> str:
    """
    Filter rows based on numeric condition.
    
    Args:
        file_path: Path to the CSV file
        column_name: Column to filter on
        operator: Comparison operator (>, <, >=, <=, ==, !=)
        value: Numeric value to compare
        
    Returns:
        String with filtering results
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        original_len = len(df)
        
        if operator == '>':
            df_filtered = df[df[column_name] > value]
        elif operator == '<':
            df_filtered = df[df[column_name] < value]
        elif operator == '>=':
            df_filtered = df[df[column_name] >= value]
        elif operator == '<=':
            df_filtered = df[df[column_name] <= value]
        elif operator == '==':
            df_filtered = df[df[column_name] == value]
        elif operator == '!=':
            df_filtered = df[df[column_name] != value]
        else:
            return f"Unknown operator: {operator}"
        
        output_path = file_path.replace('.csv', '_filtered.csv')
        df_filtered.to_csv(output_path, index=False)
        
        return f"Filtered {original_len} -> {len(df_filtered)} rows where {column_name} {operator} {value}. Saved to: {output_path}"
    except Exception as e:
        return f"Error filtering: {str(e)}"


@tool
def filter_rows_categorical(file_path: str, column_name: str, values: str, include: bool = True) -> str:
    """
    Filter rows based on categorical values.
    
    Args:
        file_path: Path to the CSV file
        column_name: Column to filter on
        values: Comma-separated list of values
        include: If True, keep rows with these values. If False, exclude them
        
    Returns:
        String with filtering results
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        value_list = [v.strip() for v in values.split(',')]
        original_len = len(df)
        
        if include:
            df_filtered = df[df[column_name].isin(value_list)]
            action = "included"
        else:
            df_filtered = df[~df[column_name].isin(value_list)]
            action = "excluded"
        
        output_path = file_path.replace('.csv', '_filtered.csv')
        df_filtered.to_csv(output_path, index=False)
        
        return f"Filtered {original_len} -> {len(df_filtered)} rows. {action}: {value_list}. Saved to: {output_path}"
    except Exception as e:
        return f"Error filtering: {str(e)}"


@tool
def select_columns(file_path: str, columns: str) -> str:
    """
    Select specific columns from the dataset.
    
    Args:
        file_path: Path to the CSV file
        columns: Comma-separated list of column names to keep
        
    Returns:
        String confirming column selection
    """
    try:
        df = pd.read_csv(file_path)
        col_list = [c.strip() for c in columns.split(',')]
        
        missing = [c for c in col_list if c not in df.columns]
        if missing:
            return f"Columns not found: {missing}"
        
        df_selected = df[col_list]
        
        output_path = file_path.replace('.csv', '_selected.csv')
        df_selected.to_csv(output_path, index=False)
        
        return f"Selected {len(col_list)} columns. New shape: {df_selected.shape}. Saved to: {output_path}"
    except Exception as e:
        return f"Error selecting columns: {str(e)}"


# ============================================================================
# STATISTICAL ANALYSIS TOOLS
# ============================================================================

@tool
def calculate_correlation(file_path: str, column1: str, column2: str, method: str = "pearson") -> str:
    """
    Calculate correlation between two numeric columns.
    
    Args:
        file_path: Path to the CSV file
        column1: First column name
        column2: Second column name
        method: 'pearson' or 'spearman'
        
    Returns:
        String with correlation coefficient and p-value
    """
    try:
        df = pd.read_csv(file_path)
        
        for col in [column1, column2]:
            if col not in df.columns:
                return f"Column '{col}' not found"
        
        # Remove rows with missing values
        data = df[[column1, column2]].dropna()
        
        if method == "pearson":
            corr, p_value = pearsonr(data[column1], data[column2])
        elif method == "spearman":
            corr, p_value = spearmanr(data[column1], data[column2])
        else:
            return f"Unknown method: {method}"
        
        interpretation = ""
        abs_corr = abs(corr)
        if abs_corr < 0.3:
            strength = "weak"
        elif abs_corr < 0.7:
            strength = "moderate"
        else:
            strength = "strong"
        
        direction = "positive" if corr > 0 else "negative"
        
        return f"{method.capitalize()} correlation between '{column1}' and '{column2}': {corr:.4f} (p-value: {p_value:.4f}). This is a {strength} {direction} correlation."
    except Exception as e:
        return f"Error calculating correlation: {str(e)}"


@tool
def perform_ttest(file_path: str, column_name: str, group_column: str) -> str:
    """
    Perform t-test between two groups.
    
    Args:
        file_path: Path to the CSV file
        column_name: Numeric column to test
        group_column: Column with two groups
        
    Returns:
        String with t-statistic and p-value
    """
    try:
        df = pd.read_csv(file_path)
        
        if column_name not in df.columns or group_column not in df.columns:
            return "Column not found"
        
        groups = df[group_column].unique()
        if len(groups) != 2:
            return f"T-test requires exactly 2 groups. Found {len(groups)}: {groups}"
        
        group1 = df[df[group_column] == groups[0]][column_name].dropna()
        group2 = df[df[group_column] == groups[1]][column_name].dropna()
        
        t_stat, p_value = stats.ttest_ind(group1, group2)
        
        mean1, mean2 = group1.mean(), group2.mean()
        
        sig = "statistically significant" if p_value < 0.05 else "not statistically significant"
        
        return f"T-test for '{column_name}' between groups:\n  {groups[0]}: mean={mean1:.2f}, n={len(group1)}\n  {groups[1]}: mean={mean2:.2f}, n={len(group2)}\n  t-statistic: {t_stat:.4f}\n  p-value: {p_value:.4f}\n  Result: {sig} (Î±=0.05)"
    except Exception as e:
        return f"Error performing t-test: {str(e)}"


@tool
def chi_square_test(file_path: str, column1: str, column2: str) -> str:
    """
    Perform chi-square test between two categorical variables.
    
    Args:
        file_path: Path to the CSV file
        column1: First categorical column
        column2: Second categorical column
        
    Returns:
        String with chi-square statistic and p-value
    """
    try:
        df = pd.read_csv(file_path)
        
        if column1 not in df.columns or column2 not in df.columns:
            return "Column not found"
        
        # Create contingency table
        contingency = pd.crosstab(df[column1], df[column2])
        
        chi2, p_value, dof, expected = chi2_contingency(contingency)
        
        sig = "dependent" if p_value < 0.05 else "independent"
        
        return f"Chi-square test between '{column1}' and '{column2}':\n  Chi-square statistic: {chi2:.4f}\n  p-value: {p_value:.4f}\n  Degrees of freedom: {dof}\n  Result: Variables are {sig} (Î±=0.05)"
    except Exception as e:
        return f"Error performing chi-square test: {str(e)}"


@tool
def calculate_group_statistics(file_path: str, value_column: str, group_column: str) -> str:
    """
    Calculate statistics for each group.
    
    Args:
        file_path: Path to the CSV file
        value_column: Numeric column to analyze
        group_column: Column to group by
        
    Returns:
        String with statistics for each group
    """
    try:
        df = pd.read_csv(file_path)
        
        if value_column not in df.columns or group_column not in df.columns:
            return "Column not found"
        
        grouped = df.groupby(group_column)[value_column].agg([
            'count', 'mean', 'std', 'min', 'max'
        ])
        
        result = f"Group statistics for '{value_column}' by '{group_column}':\n"
        result += grouped.to_string()
        
        return result
    except Exception as e:
        return f"Error calculating group statistics: {str(e)}"


# ============================================================================
# DATA VISUALIZATION TOOLS
# ============================================================================

@tool
def create_histogram(file_path: str, column_name: str, bins: int = 20) -> str:
    """
    Create histogram for a numeric column.
    
    Args:
        file_path: Path to the CSV file
        column_name: Column to plot
        bins: Number of bins
        
    Returns:
        String confirming plot creation
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        plt.figure(figsize=(10, 6))
        plt.hist(df[column_name].dropna(), bins=bins, edgecolor='black', alpha=0.7)
        plt.title(f'Histogram of {column_name}')
        plt.xlabel(column_name)
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        
        output_path = file_path.replace('.csv', f'_histogram_{column_name}.png')
        plt.savefig(output_path, dpi=100, bbox_inches='tight')
        plt.close()
        
        # Calculate statistics
        data = df[column_name].dropna()
        stats_info = f"Mean: {data.mean():.2f}, Median: {data.median():.2f}, Std: {data.std():.2f}"
        
        return f"Histogram created for '{column_name}'. {stats_info}. Saved to: {output_path}"
    except Exception as e:
        return f"Error creating histogram: {str(e)}"


@tool
def create_scatter_plot(file_path: str, x_column: str, y_column: str, color_column: Optional[str] = None) -> str:
    """
    Create scatter plot between two numeric columns.
    
    Args:
        file_path: Path to the CSV file
        x_column: Column for x-axis
        y_column: Column for y-axis
        color_column: Optional column for color coding
        
    Returns:
        String confirming plot creation
    """
    try:
        df = pd.read_csv(file_path)
        
        for col in [x_column, y_column]:
            if col not in df.columns:
                return f"Column '{col}' not found"
        
        plt.figure(figsize=(10, 6))
        
        if color_column and color_column in df.columns:
            for category in df[color_column].unique():
                mask = df[color_column] == category
                plt.scatter(df[mask][x_column], df[mask][y_column], label=category, alpha=0.6)
            plt.legend()
        else:
            plt.scatter(df[x_column], df[y_column], alpha=0.6)
        
        plt.title(f'Scatter Plot: {x_column} vs {y_column}')
        plt.xlabel(x_column)
        plt.ylabel(y_column)
        plt.grid(True, alpha=0.3)
        
        output_path = file_path.replace('.csv', f'_scatter_{x_column}_{y_column}.png')
        plt.savefig(output_path, dpi=100, bbox_inches='tight')
        plt.close()
        
        # Calculate correlation
        corr = df[[x_column, y_column]].corr().iloc[0, 1]
        
        return f"Scatter plot created. Correlation: {corr:.3f}. Saved to: {output_path}"
    except Exception as e:
        return f"Error creating scatter plot: {str(e)}"


@tool
def create_bar_chart(file_path: str, column_name: str, top_n: int = 10) -> str:
    """
    Create bar chart for categorical column.
    
    Args:
        file_path: Path to the CSV file
        column_name: Categorical column to plot
        top_n: Number of top categories to show
        
    Returns:
        String confirming plot creation
    """
    try:
        df = pd.read_csv(file_path)
        if column_name not in df.columns:
            return f"Column '{column_name}' not found"
        
        value_counts = df[column_name].value_counts().head(top_n)
        
        plt.figure(figsize=(10, 6))
        value_counts.plot(kind='bar', edgecolor='black', alpha=0.7)
        plt.title(f'Bar Chart: Top {top_n} values in {column_name}')
        plt.xlabel(column_name)
        plt.ylabel('Count')
        plt.xticks(rotation=45, ha='right')
        plt.grid(True, alpha=0.3, axis='y')
        
        output_path = file_path.replace('.csv', f'_bar_{column_name}.png')
        plt.savefig(output_path, dpi=100, bbox_inches='tight')
        plt.close()
        
        return f"Bar chart created for '{column_name}'. Top value: {value_counts.index[0]} ({value_counts.iloc[0]} occurrences). Saved to: {output_path}"
    except Exception as e:
        return f"Error creating bar chart: {str(e)}"


@tool
def create_correlation_heatmap(file_path: str, columns: Optional[str] = None) -> str:
    """
    Create correlation heatmap for numeric columns.
    
    Args:
        file_path: Path to the CSV file
        columns: Optional comma-separated list of columns (if None, uses all numeric)
        
    Returns:
        String confirming plot creation
    """
    try:
        df = pd.read_csv(file_path)
        
        if columns:
            col_list = [c.strip() for c in columns.split(',')]
            df_numeric = df[col_list]
        else:
            df_numeric = df.select_dtypes(include=[np.number])
        
        if df_numeric.shape[1] < 2:
            return "Need at least 2 numeric columns for correlation heatmap"
        
        plt.figure(figsize=(12, 8))
        corr_matrix = df_numeric.corr()
        
        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, 
                    square=True, linewidths=1, cbar_kws={"shrink": 0.8})
        plt.title('Correlation Heatmap')
        
        output_path = file_path.replace('.csv', '_correlation_heatmap.png')
        plt.savefig(output_path, dpi=100, bbox_inches='tight')
        plt.close()
        
        # Find strongest correlations
        upper_tri = np.triu(np.ones_like(corr_matrix), k=1)
        upper_tri_corr = corr_matrix.where(upper_tri.astype(bool))
        strongest = upper_tri_corr.abs().unstack().nlargest(3)
        
        return f"Correlation heatmap created for {corr_matrix.shape[0]} variables. Saved to: {output_path}"
    except Exception as e:
        return f"Error creating heatmap: {str(e)}"


# ============================================================================
# ENHANCED MACHINE LEARNING TOOLS
# ============================================================================

@tool
def train_random_forest_model(file_path: str, target_column: str, feature_columns: str, task_type: str = "classification") -> str:
    """
    Train a Random Forest model.

    Args:
        file_path: Path to the data file
        target_column: Name of the target variable column
        feature_columns: Comma-separated list of feature columns
        task_type: Type of task (classification, regression)

    Returns:
        String describing the model training results including feature importance
    """
    try:
        df = pd.read_csv(file_path)

        # Parse feature columns
        features = [col.strip() for col in feature_columns.split(',')]

        # Check if columns exist
        for col in features + [target_column]:
            if col not in df.columns:
                return f"Column '{col}' not found in dataset"

        # Prepare data
        X = df[features]
        y = df[target_column]

        # Handle missing values
        X = X.fillna(X.mean())
        if task_type == "classification":
            y = y.fillna(y.mode()[0])
        else:
            y = y.fillna(y.mean())

        # Handle categorical variables
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))

        if task_type == "classification" and y.dtype == 'object':
            le_target = LabelEncoder()
            y = le_target.fit_transform(y.astype(str))

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train model
        if task_type == "classification":
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            scoring_metric = 'accuracy'
        else:
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            scoring_metric = 'r2'

        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate metrics
        if task_type == "classification":
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

            metrics = {
                "accuracy": round(accuracy, 4),
                "precision": round(precision, 4),
                "recall": round(recall, 4),
                "f1_score": round(f1, 4)
            }
        else:
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            metrics = {
                "mse": round(mse, 4),
                "r2_score": round(r2, 4)
            }

        # Feature importance
        feature_importance = dict(zip(features, [round(imp, 4) for imp in model.feature_importances_]))

        # Cross-validation
        cv_scores = cross_val_score(model, X, y, cv=5, scoring=scoring_metric)

        # Save model
        model_path = file_path.replace('.csv', f'_rf_{task_type}_model.joblib')
        joblib.dump(model, model_path)

        results = {
            "model_type": f"Random Forest {task_type}",
            "features": features,
            "target": target_column,
            "train_size": len(X_train),
            "test_size": len(X_test),
            **metrics,
            "cv_score_mean": round(cv_scores.mean(), 4),
            "cv_score_std": round(cv_scores.std(), 4),
            "feature_importance": feature_importance,
            "model_saved": model_path
        }

        return f"Random Forest model trained: {results}"

    except Exception as e:
        return f"Error training Random Forest model: {str(e)}"


@tool
def train_knn_model(file_path: str, target_column: str, feature_columns: str, task_type: str = "classification", n_neighbors: int = 5) -> str:
    """
    Train a K-Nearest Neighbors model.

    Args:
        file_path: Path to the data file
        target_column: Name of the target variable column
        feature_columns: Comma-separated list of feature columns
        task_type: Type of task (classification, regression)
        n_neighbors: Number of neighbors to use

    Returns:
        String describing the model training results
    """
    try:
        df = pd.read_csv(file_path)

        # Parse feature columns
        features = [col.strip() for col in feature_columns.split(',')]

        # Check if columns exist
        for col in features + [target_column]:
            if col not in df.columns:
                return f"Column '{col}' not found in dataset"

        # Prepare data
        X = df[features]
        y = df[target_column]

        # Handle missing values
        X = X.fillna(X.mean())
        if task_type == "classification":
            y = y.fillna(y.mode()[0])
        else:
            y = y.fillna(y.mean())

        # Handle categorical variables
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))

        if task_type == "classification" and y.dtype == 'object':
            le_target = LabelEncoder()
            y = le_target.fit_transform(y.astype(str))

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train model
        if task_type == "classification":
            model = KNeighborsClassifier(n_neighbors=n_neighbors)
            scoring_metric = 'accuracy'
        else:
            model = KNeighborsRegressor(n_neighbors=n_neighbors)
            scoring_metric = 'r2'

        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate metrics
        if task_type == "classification":
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

            metrics = {
                "accuracy": round(accuracy, 4),
                "precision": round(precision, 4),
                "recall": round(recall, 4),
                "f1_score": round(f1, 4)
            }
        else:
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            metrics = {
                "mse": round(mse, 4),
                "r2_score": round(r2, 4)
            }

        # Cross-validation
        cv_scores = cross_val_score(model, X, y, cv=5, scoring=scoring_metric)

        # Save model
        model_path = file_path.replace('.csv', f'_knn_{task_type}_model.joblib')
        joblib.dump(model, model_path)

        results = {
            "model_type": f"KNN {task_type}",
            "n_neighbors": n_neighbors,
            "features": features,
            "target": target_column,
            "train_size": len(X_train),
            "test_size": len(X_test),
            **metrics,
            "cv_score_mean": round(cv_scores.mean(), 4),
            "cv_score_std": round(cv_scores.std(), 4),
            "model_saved": model_path
        }

        return f"KNN model trained: {results}"

    except Exception as e:
        return f"Error training KNN model: {str(e)}"


@tool
def train_svm_model(file_path: str, target_column: str, feature_columns: str, task_type: str = "classification") -> str:
    """
    Train a Support Vector Machine model.

    Args:
        file_path: Path to the data file
        target_column: Name of the target variable column
        feature_columns: Comma-separated list of feature columns
        task_type: Type of task (classification, regression)

    Returns:
        String describing the model training results
    """
    try:
        df = pd.read_csv(file_path)

        # Parse feature columns
        features = [col.strip() for col in feature_columns.split(',')]

        # Check if columns exist
        for col in features + [target_column]:
            if col not in df.columns:
                return f"Column '{col}' not found in dataset"

        # Prepare data
        X = df[features]
        y = df[target_column]

        # Handle missing values
        X = X.fillna(X.mean())
        if task_type == "classification":
            y = y.fillna(y.mode()[0])
        else:
            y = y.fillna(y.mean())

        # Handle categorical variables
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))

        if task_type == "classification" and y.dtype == 'object':
            le_target = LabelEncoder()
            y = le_target.fit_transform(y.astype(str))

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train model
        if task_type == "classification":
            model = SVC(kernel='rbf', random_state=42)
            scoring_metric = 'accuracy'
        else:
            model = SVR(kernel='rbf')
            scoring_metric = 'r2'

        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate metrics
        if task_type == "classification":
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

            metrics = {
                "accuracy": round(accuracy, 4),
                "precision": round(precision, 4),
                "recall": round(recall, 4),
                "f1_score": round(f1, 4)
            }
        else:
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            metrics = {
                "mse": round(mse, 4),
                "r2_score": round(r2, 4)
            }

        # Cross-validation
        cv_scores = cross_val_score(model, X, y, cv=5, scoring=scoring_metric)

        # Save model
        model_path = file_path.replace('.csv', f'_svm_{task_type}_model.joblib')
        joblib.dump(model, model_path)

        results = {
            "model_type": f"SVM {task_type}",
            "features": features,
            "target": target_column,
            "train_size": len(X_train),
            "test_size": len(X_test),
            **metrics,
            "cv_score_mean": round(cv_scores.mean(), 4),
            "cv_score_std": round(cv_scores.std(), 4),
            "model_saved": model_path
        }

        return f"SVM model trained: {results}"

    except Exception as e:
        return f"Error training SVM model: {str(e)}"


@tool
def evaluate_model(model_path: str, test_data_path: str, target_column: str, feature_columns: str) -> str:
    """
    Evaluate a trained model on new test data.

    Args:
        model_path: Path to the saved model file
        test_data_path: Path to the test data file
        target_column: Name of the target variable column
        feature_columns: Comma-separated list of feature columns

    Returns:
        String describing the model evaluation results
    """
    try:
        # Load the model
        model = joblib.load(model_path)

        # Load test data
        df = pd.read_csv(test_data_path)

        # Parse feature columns
        features = [col.strip() for col in feature_columns.split(',')]

        # Check if columns exist
        for col in features + [target_column]:
            if col not in df.columns:
                return f"Column '{col}' not found in test dataset"

        # Prepare test data
        X = df[features]
        y = df[target_column]

        # Handle missing values
        X = X.fillna(X.mean())

        # Handle categorical variables (same as training)
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))

        # Make predictions
        y_pred = model.predict(X)

        # Determine if this is classification or regression based on model type
        model_name = str(type(model).__name__)
        is_classification = any(clf in model_name for clf in ['Classifier', 'SVC'])

        # Calculate appropriate metrics
        if is_classification:
            accuracy = accuracy_score(y, y_pred)
            precision = precision_score(y, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y, y_pred, average='weighted', zero_division=0)

            results = {
                "model_type": model_name,
                "task": "classification",
                "test_samples": len(X),
                "accuracy": round(accuracy, 4),
                "precision": round(precision, 4),
                "recall": round(recall, 4),
                "f1_score": round(f1, 4)
            }
        else:
            mse = mean_squared_error(y, y_pred)
            r2 = r2_score(y, y_pred)
            rmse = np.sqrt(mse)

            results = {
                "model_type": model_name,
                "task": "regression",
                "test_samples": len(X),
                "mse": round(mse, 4),
                "rmse": round(rmse, 4),
                "r2_score": round(r2, 4)
            }

        return f"Model evaluation results: {results}"

    except Exception as e:
        return f"Error evaluating model: {str(e)}"



@tool
def train_logistic_regression(file_path: str, target_column: str, feature_columns: str) -> str:
    """
    Train logistic regression for binary classification.
    
    Args:
        file_path: Path to the CSV file
        target_column: Target variable column
        feature_columns: Comma-separated list of feature columns
        
    Returns:
        String with model performance metrics
    """
    try:
        df = pd.read_csv(file_path)
        features = [col.strip() for col in feature_columns.split(',')]
        
        # Prepare data
        X = df[features]
        y = df[target_column]
        
        # Handle categorical features
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].fillna('missing'))
        
        # Handle missing values
        X = X.fillna(X.mean())
        
        # Encode target if categorical
        if y.dtype == 'object':
            le_target = LabelEncoder()
            y = le_target.fit_transform(y)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train model
        model = LogisticRegression(max_iter=1000, random_state=42)
        model.fit(X_train, y_train)
        
        # Predictions
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
        
        # Metrics
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred, average='binary')
        rec = recall_score(y_test, y_pred, average='binary')
        f1 = f1_score(y_test, y_pred, average='binary')
        
        # Check if binary classification for AUC
        if len(np.unique(y)) == 2:
            auc = roc_auc_score(y_test, y_proba)
            auc_str = f", AUC: {auc:.3f}"
        else:
            auc_str = ""
        
        # Feature importance (coefficients)
        importance = dict(zip(features, model.coef_[0]))
        top_features = sorted(importance.items(), key=lambda x: abs(x[1]), reverse=True)[:3]
        
        # Save model
        model_path = file_path.replace('.csv', '_logistic_model.joblib')
        joblib.dump(model, model_path)
        
        return f"Logistic Regression trained:\n  Accuracy: {acc:.3f}\n  Precision: {prec:.3f}\n  Recall: {rec:.3f}\n  F1: {f1:.3f}{auc_str}\n  Top features: {top_features}\n  Model saved: {model_path}"
    except Exception as e:
        return f"Error training logistic regression: {str(e)}"


@tool
def train_decision_tree(file_path: str, target_column: str, feature_columns: str, max_depth: int = 5) -> str:
    """
    Train decision tree classifier.
    
    Args:
        file_path: Path to the CSV file
        target_column: Target variable column
        feature_columns: Comma-separated list of feature columns
        max_depth: Maximum depth of the tree
        
    Returns:
        String with model performance and feature importance
    """
    try:
        df = pd.read_csv(file_path)
        features = [col.strip() for col in feature_columns.split(',')]
        
        # Prepare data
        X = df[features]
        y = df[target_column]
        
        # Handle categorical features
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].fillna('missing'))
        
        X = X.fillna(X.mean())
        
        if y.dtype == 'object':
            le_target = LabelEncoder()
            y = le_target.fit_transform(y)
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train model
        model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
        model.fit(X_train, y_train)
        
        # Predictions
        y_pred = model.predict(X_test)
        
        # Metrics
        acc = accuracy_score(y_test, y_pred)
        
        # Feature importance
        importance = dict(zip(features, model.feature_importances_))
        top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # Save model
        model_path = file_path.replace('.csv', '_decision_tree.joblib')
        joblib.dump(model, model_path)
        
        return f"Decision Tree trained:\n  Accuracy: {acc:.3f}\n  Max depth: {max_depth}\n  Top features: {top_features}\n  Model saved: {model_path}"
    except Exception as e:
        return f"Error training decision tree: {str(e)}"


@tool
def train_gradient_boosting(file_path: str, target_column: str, feature_columns: str, n_estimators: int = 100) -> str:
    """
    Train gradient boosting classifier.
    
    Args:
        file_path: Path to the CSV file
        target_column: Target variable column
        feature_columns: Comma-separated list of feature columns
        n_estimators: Number of boosting stages
        
    Returns:
        String with model performance
    """
    try:
        df = pd.read_csv(file_path)
        features = [col.strip() for col in feature_columns.split(',')]
        
        # Prepare data
        X = df[features]
        y = df[target_column]
        
        # Handle categorical features
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].fillna('missing'))
        
        X = X.fillna(X.mean())
        
        if y.dtype == 'object':
            le_target = LabelEncoder()
            y = le_target.fit_transform(y)
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train model
        model = GradientBoostingClassifier(n_estimators=n_estimators, random_state=42)
        model.fit(X_train, y_train)
        
        # Predictions
        y_pred = model.predict(X_test)
        
        # Metrics
        acc = accuracy_score(y_test, y_pred)
        
        # Feature importance
        importance = dict(zip(features, model.feature_importances_))
        top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # Save model
        model_path = file_path.replace('.csv', '_gradient_boosting.joblib')
        joblib.dump(model, model_path)
        
        return f"Gradient Boosting trained:\n  Accuracy: {acc:.3f}\n  N estimators: {n_estimators}\n  Top features: {top_features}\n  Model saved: {model_path}"
    except Exception as e:
        return f"Error training gradient boosting: {str(e)}"


@tool
def make_prediction(model_path: str, data_path: str, feature_columns: str) -> str:
    """
    Make predictions using a trained model.
    
    Args:
        model_path: Path to saved model
        data_path: Path to data for prediction
        feature_columns: Comma-separated list of feature columns
        
    Returns:
        String with predictions
    """
    try:
        model = joblib.load(model_path)
        df = pd.read_csv(data_path)
        features = [col.strip() for col in feature_columns.split(',')]
        
        X = df[features]
        
        # Handle categorical features
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].fillna('missing'))
        
        X = X.fillna(X.mean())
        
        # Make predictions
        predictions = model.predict(X)
        
        # Add predictions to dataframe
        df['prediction'] = predictions
        
        # If probabilistic model, add probabilities
        if hasattr(model, 'predict_proba'):
            probas = model.predict_proba(X)
            if probas.shape[1] == 2:
                df['probability'] = probas[:, 1]
        
        output_path = data_path.replace('.csv', '_predictions.csv')
        df.to_csv(output_path, index=False)
        
        # Summary
        unique, counts = np.unique(predictions, return_counts=True)
        pred_summary = dict(zip(unique, counts))
        
        return f"Predictions made for {len(predictions)} samples:\n  Distribution: {pred_summary}\n  Results saved to: {output_path}"
    except Exception as e:
        return f"Error making predictions: {str(e)}"


@tool
def perform_cross_validation(file_path: str, target_column: str, feature_columns: str, model_type: str = "random_forest", cv_folds: int = 5) -> str:
    """
    Perform cross-validation for model evaluation.
    
    Args:
        file_path: Path to the CSV file
        target_column: Target variable column
        feature_columns: Comma-separated list of feature columns
        model_type: Type of model (random_forest, logistic, svm)
        cv_folds: Number of cross-validation folds
        
    Returns:
        String with cross-validation scores
    """
    try:
        df = pd.read_csv(file_path)
        features = [col.strip() for col in feature_columns.split(',')]
        
        # Prepare data
        X = df[features]
        y = df[target_column]
        
        # Handle categorical features
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].fillna('missing'))
        
        X = X.fillna(X.mean())
        
        if y.dtype == 'object':
            le_target = LabelEncoder()
            y = le_target.fit_transform(y)
        
        # Select model
        if model_type == "random_forest":
            model = RandomForestClassifier(n_estimators=100, random_state=42)
        elif model_type == "logistic":
            model = LogisticRegression(max_iter=1000, random_state=42)
        elif model_type == "svm":
            model = SVC(random_state=42)
        else:
            return f"Unknown model type: {model_type}"
        
        # Perform cross-validation
        cv_scores = cross_val_score(model, X, y, cv=cv_folds, scoring='accuracy')
        
        return f"Cross-validation results ({cv_folds} folds):\n  Model: {model_type}\n  Scores: {cv_scores.round(3)}\n  Mean: {cv_scores.mean():.3f}\n  Std: {cv_scores.std():.3f}\n  Min: {cv_scores.min():.3f}\n  Max: {cv_scores.max():.3f}"
    except Exception as e:
        return f"Error in cross-validation: {str(e)}"


@tool
def feature_selection(file_path: str, target_column: str, feature_columns: str, n_features: int = 5) -> str:
    """
    Select top features based on importance.
    
    Args:
        file_path: Path to the CSV file
        target_column: Target variable column
        feature_columns: Comma-separated list of feature columns
        n_features: Number of top features to select
        
    Returns:
        String with selected features and their scores
    """
    try:
        df = pd.read_csv(file_path)
        features = [col.strip() for col in feature_columns.split(',')]
        
        # Prepare data
        X = df[features]
        y = df[target_column]
        
        # Handle categorical features
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].fillna('missing'))
        
        X = X.fillna(X.mean())
        
        if y.dtype == 'object':
            le_target = LabelEncoder()
            y = le_target.fit_transform(y)
        
        # Feature selection
        selector = SelectKBest(score_func=f_classif, k=min(n_features, len(features)))
        X_selected = selector.fit_transform(X, y)
        
        # Get selected features
        selected_mask = selector.get_support()
        selected_features = [f for f, s in zip(features, selected_mask) if s]
        
        # Get scores
        feature_scores = dict(zip(features, selector.scores_))
        top_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)[:n_features]
        
        # Save selected features dataset
        df_selected = pd.concat([df[selected_features], df[target_column]], axis=1)
        output_path = file_path.replace('.csv', '_selected_features.csv')
        df_selected.to_csv(output_path, index=False)
        
        return f"Feature selection complete:\n  Selected {len(selected_features)} features: {selected_features}\n  Top scores: {top_features}\n  Saved to: {output_path}"
    except Exception as e:
        return f"Error in feature selection: {str(e)}"


# ============================================================================
# ANSWER GENERATION TOOLS
# ============================================================================

@tool
def answer_survival_question(file_path: str, passenger_info: str) -> str:
    """
    Answer survival questions about Titanic passengers.
    
    Args:
        file_path: Path to the CSV file
        passenger_info: Description of passenger (e.g., "male, age 30, first class")
        
    Returns:
        String with survival prediction and analysis
    """
    try:
        df = pd.read_csv(file_path)
        
        # Parse passenger info
        info_lower = passenger_info.lower()
        
        # Extract features
        conditions = []
        
        if 'male' in info_lower or 'man' in info_lower:
            conditions.append(df['Sex'] == 'male')
        elif 'female' in info_lower or 'woman' in info_lower:
            conditions.append(df['Sex'] == 'female')
        
        if 'first class' in info_lower or 'class 1' in info_lower:
            conditions.append(df['Pclass'] == 1)
        elif 'second class' in info_lower or 'class 2' in info_lower:
            conditions.append(df['Pclass'] == 2)
        elif 'third class' in info_lower or 'class 3' in info_lower:
            conditions.append(df['Pclass'] == 3)
        
        # Age extraction
        import re
        age_match = re.search(r'age (\d+)', info_lower)
        if age_match:
            age = int(age_match.group(1))
            conditions.append((df['Age'] >= age - 5) & (df['Age'] <= age + 5))
        
        # Filter data
        if conditions:
            mask = conditions[0]
            for cond in conditions[1:]:
                mask = mask & cond
            filtered = df[mask]
        else:
            filtered = df
        
        if len(filtered) == 0:
            return "No passengers found matching the description"
        
        # Calculate survival rate
        survival_rate = filtered['Survived'].mean()
        total_matching = len(filtered)
        survived = filtered['Survived'].sum()
        
        # Additional context
        overall_survival = df['Survived'].mean()
        
        return f"Survival analysis for '{passenger_info}':\n  Matching passengers: {total_matching}\n  Survived: {survived}\n  Survival rate: {survival_rate:.1%}\n  Overall survival rate: {overall_survival:.1%}\n  Relative survival: {survival_rate/overall_survival:.2f}x the average"
    except Exception as e:
        return f"Error analyzing survival: {str(e)}"


@tool
def get_dataset_insights(file_path: str) -> str:
    """
    Generate comprehensive insights about the dataset.
    
    Args:
        file_path: Path to the CSV file
        
    Returns:
        String with key insights about the data
    """
    try:
        df = pd.read_csv(file_path)
        
        insights = []
        
        # Basic info
        insights.append(f"Dataset has {len(df)} rows and {len(df.columns)} columns")
        
        # Missing data
        missing = df.isnull().sum()
        if missing.sum() > 0:
            worst_missing = missing.nlargest(3)
            insights.append(f"Missing data in: {worst_missing.to_dict()}")
        
        # For Titanic specific insights
        if 'Survived' in df.columns:
            survival_rate = df['Survived'].mean()
            insights.append(f"Overall survival rate: {survival_rate:.1%}")
            
            if 'Sex' in df.columns:
                female_survival = df[df['Sex'] == 'female']['Survived'].mean()
                male_survival = df[df['Sex'] == 'male']['Survived'].mean()
                insights.append(f"Female survival: {female_survival:.1%}, Male survival: {male_survival:.1%}")
            
            if 'Pclass' in df.columns:
                class_survival = df.groupby('Pclass')['Survived'].mean()
                insights.append(f"Survival by class: {class_survival.to_dict()}")
        
        # Numeric columns
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        insights.append(f"Numeric columns: {list(numeric_cols)}")
        
        # Categorical columns
        cat_cols = df.select_dtypes(include=['object']).columns
        insights.append(f"Categorical columns: {list(cat_cols)}")
        
        return "Dataset Insights:\n" + "\n".join(f"â€¢ {insight}" for insight in insights)
    except Exception as e:
        return f"Error generating insights: {str(e)}"




# ============================================================================
# SPECIALIZED AGENTS
# ============================================================================

def create_agents():
    """Create and return all specialized agents."""

    print("ðŸ”§ Initializing Model...")
    # Initialize the LLM model
    # model = TransformersModel(
    #     model_id=CONFIG["model_id"],
    #     temperature=CONFIG["temperature"],
    #     max_new_tokens=CONFIG["max_new_tokens"],
    #     trust_remote_code=True,
    #     token=CONFIG["huggingface_token"]
    # )
    # model = InferenceClientModel(
    #     model_id=CONFIG["model_id"],
    #     token=CONFIG["huggingface_token"]
    # )
    print(f"âœ… Model initialized: {CONFIG['model_id']}")

    print("\nðŸ¤– Creating Specialized Agents...")

    # Data Reader Agent
    print("   ðŸ“Š Creating Data Reader Agent...")
    # data_reader_agent = CodeAgent(
    #     tools=[
    #         read_csv_file,
    #         read_json_file,
    #         get_column_info,
    #         get_data_summary,
    #         preview_data
    #     ],
    #     model=model,
    #     name="data_reader",
    #     description="Analyzes datasets, reads CSV/JSON files, provides column information, statistical summaries, and data previews. Use this agent for data exploration and understanding data structure.",
    #     max_steps=10,
    #     verbosity_level=1
    # )

    # Data Manipulation Agent
    print("   ðŸ”§ Creating Data Manipulation Agent...")
    # data_manipulation_agent = CodeAgent(
    #     tools=[
    #         handle_missing_values,
    #         create_dummy_variables,
    #         modify_column_values,
    #         convert_data_types
    #     ],
    #     model=model,
    #     name="data_manipulation",
    #     description="Handles data preprocessing, missing value imputation, categorical encoding, data type conversions, and column transformations. Use this agent for data cleaning and preparation.",
    #     max_steps=10,
    #     verbosity_level=1
    # )

    # Data Operations Agent
    print("   âš¡ Creating Data Operations Agent...")
    # data_operations_agent = CodeAgent(
    #     tools=[
    #         filter_data,
    #         perform_math_operations,
    #         aggregate_data,
    #         string_operations
    #     ],
    #     model=model,
    #     name="data_operations",
    #     description="Performs mathematical operations, data filtering, aggregations, grouping, and string manipulations. Use this agent for data analysis and feature engineering.",
    #     max_steps=10,
    #     verbosity_level=1
    # )

    # ML Prediction Agent
    # print("   ðŸŽ¯ Creating ML Prediction Agent...")
    # ml_prediction_agent = CodeAgent(
    #     tools=[
    #         train_regression_model,
    #         train_svm_model,
    #         train_random_forest_model,
    #         train_knn_model,
    #         evaluate_model
    #     ],
    #     model=model,
    #     name="ml_prediction",
    #     description="Trains and evaluates machine learning models including Linear Regression, SVM, Random Forest, and KNN. Provides model performance metrics, feature importance, and cross-validation results. Use this agent for predictive modeling and model evaluation.",
    #     max_steps=10,
    #     verbosity_level=1
    # )

    print("âœ… All specialized agents created successfully!")

    # return model, data_reader_agent, data_manipulation_agent, data_operations_agent, ml_prediction_agent


# ============================================================================
# MANAGER AGENT (ORCHESTRATOR)
# ============================================================================

def create_manager_agent():
    """Create the manager agent that coordinates all specialized agents."""

    # print("\nðŸ‘” Creating Manager Agent (Orchestrator)...")

    # Wrap specialized agents in ManagedAgent
    # managed_data_reader = ManagedAgent(
    #     agent=data_reader_agent,
    #     name="data_reader",
    #     description="Analyzes datasets, reads CSV/JSON files, provides column information, statistical summaries, and data previews. Use for data exploration and understanding data structure."
    # )

    # managed_data_manipulation = ManagedAgent(
    #     agent=data_manipulation_agent,
    #     name="data_manipulation",
    #     description="Handles data preprocessing, missing value imputation, categorical encoding, data type conversions, and column transformations. Use for data cleaning and preparation."
    # )

    # managed_data_operations = ManagedAgent(
    #     agent=data_operations_agent,
    #     name="data_operations",
    #     description="Performs mathematical operations, data filtering, aggregations, grouping, and string manipulations. Use for data analysis and feature engineering."
    # )

    # managed_ml_prediction = ManagedAgent(
    #     agent=ml_prediction_agent,
    #     name="ml_prediction",
    #     description="Trains and evaluates machine learning models including Linear Regression, SVM, Random Forest, and KNN. Provides model performance metrics, feature importance, and cross-validation results. Use for predictive modeling."
    # )
    
    model = TransformersModel(
        model_id=CONFIG["model_id"],
        temperature=CONFIG["temperature"],
        max_new_tokens=CONFIG["max_new_tokens"],
        trust_remote_code=True,
        token=CONFIG["huggingface_token"]
    )

    # Create the manager agent
    manager_agent = CodeAgent(
        tools=[
            # read_csv_file,
            # read_json_file,
            # get_column_info,
            # get_data_summary,
            # preview_data,
            # handle_missing_values,
            # create_dummy_variables,
            # modify_column_values,
            # convert_data_types,
            # filter_data,
            # perform_math_operations,
            # aggregate_data,
            # string_operations,
            # train_regression_model,
            # train_svm_model,
            # train_random_forest_model,
            # train_knn_model,
            # evaluate_model
            
            load_dataset,
            get_column_names,
            get_data_types,
            get_null_counts,
            get_unique_values,
            get_numeric_summary,
            get_first_rows,
            get_dataset_insights,
            drop_column,
            drop_null_rows,
            fill_numeric_nulls,
            fill_categorical_nulls,
            encode_categorical,
            create_new_feature,
            normalize_column,
            filter_rows_numeric,
            filter_rows_categorical,
            select_columns,
            calculate_correlation,
            perform_ttest,
            chi_square_test,
            calculate_group_statistics,
            create_histogram,
            create_scatter_plot,
            create_bar_chart,
            create_correlation_heatmap,
            train_logistic_regression,
            train_decision_tree,
            train_random_forest_model,
            train_gradient_boosting,
            train_knn_model,
            train_svm_model,
            perform_cross_validation,
            feature_selection,
            make_prediction,
            evaluate_model,
            answer_survival_question,
            ],
        model=model,
        # managed_agents=[
        #     data_reader_agent,
        #     data_manipulation_agent,
        #     data_operations_agent,
        #     ml_prediction_agent
        # ],
        additional_authorized_imports=["pandas", "numpy", "sklearn", "joblib", "csv", "json"],
        max_steps=20,
        verbosity_level=2,
        planning_interval=5
    )

    print("âœ… Manager Agent created successfully!")
    print("\nðŸŽ¯ Multi-Agent System Ready!")

    return manager_agent


# ============================================================================
# ORCHESTRATION FUNCTION
# ============================================================================

def run_analysis(user_prompt: str, file_path: str = None) -> str:
    """
    Run multi-agent data analysis based on user prompt.

    Args:
        user_prompt: User's analysis request in natural language
        file_path: Path to the data file (optional, uses default if not provided)

    Returns:
        String containing the analysis results
    """
    # Use default dataset if no file path provided
    if file_path is None:
        file_path = CONFIG["default_dataset"]

    print(f"\n{'='*80}")
    print(f"ðŸš€ Starting Multi-Agent Analysis")
    print(f"{'='*80}")
    print(f"ðŸ“ User Prompt: {user_prompt}")
    print(f"ðŸ“ Data File: {file_path}")
    print(f"{'='*80}\n")

    # Create agents
    # model, data_reader, data_manipulation, data_operations, ml_prediction = create_agents()

    # Create manager agent
    manager = create_manager_agent()

    # Format the task for the manager
    task = f"""
Task: {user_prompt}
Data file: {file_path}
"""
# IMPORTANT INSTRUCTIONS:
# - You MUST ONLY use the tools and agents that have been explicitly provided to you
# - DO NOT invent, create, or imagine new tools, functions, or agents
# - DO NOT attempt to use tools or methods that are not in the provided list
# - If you cannot complete a task with the available tools, state this clearly

# You have access to ONLY these specialized agents:
# - data_reader: For reading and exploring the dataset (tools: read_csv_file, read_json_file, get_column_info, get_data_summary, preview_data)
# - data_manipulation: For data cleaning and preprocessing (tools: handle_missing_values, create_dummy_variables, modify_column_values, convert_data_types)
# - data_operations: For mathematical operations and aggregations (tools: filter_data, perform_math_operations, aggregate_data, string_operations)
# - ml_prediction: For training and evaluating machine learning models (tools: train_regression_model, train_svm_model, train_random_forest_model, train_knn_model, evaluate_model)

# Analyze the task and delegate work ONLY to the appropriate agents listed above in the correct order.
# Use ONLY the tools available to each agent.
# Provide a comprehensive final answer with all results.


    print("ðŸŽ¬ Executing analysis...\n")

    try:
        # Run the manager agent
        result = manager.run(task)

        print(f"\n{'='*80}")
        print("âœ… Analysis Complete!")
        print(f"{'='*80}\n")

        return result

    except Exception as e:
        error_msg = f"âŒ Error during analysis: {str(e)}"
        print(f"\n{error_msg}\n")
        return error_msg


# ============================================================================
# MAIN ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                            â•‘
â•‘             ðŸ¤– MULTI-AGENT DATA ANALYSIS SYSTEM (smolagents)               â•‘
â•‘                                                                            â•‘
â•‘  A comprehensive multi-agent system for end-to-end data science workflows â•‘
â•‘                                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    print("\nðŸŽ¯ Interactive Mode")
    print("="*80)

    # Get file path from user
    print("\nðŸ“ Enter the path to your data file (or press Enter for default 'data/titanic.csv'):")
    file_input = input("File path: ").strip()
    file_path = file_input if file_input else "data/titanic.csv"

    print(f"\nâœ“ Using file: {file_path}")

    # Get analysis request from user
    print("\nðŸ“ Enter your analysis request:")
    print("   (Examples: 'Analyze the dataset', 'Handle missing values', 'Train a model', etc.)")
    print("   Type your request below (press Enter twice to finish):\n")

    lines = []
    while True:
        line = input()
        if line:
            lines.append(line)
        else:
            if lines:  # If we have at least one line and get an empty line, we're done
                break
            else:  # If first line is empty, ask again
                continue

    user_prompt = "\n".join(lines)

    if not user_prompt.strip():
        print("\nâš ï¸  No request provided. Using default example...")
        user_prompt = "Provide a comprehensive overview of the dataset including structure, missing values, and statistical summary"

    # Run the analysis
    print("\n" + "="*80)
    print("ðŸš€ Starting Analysis")
    print("="*80)

    result = run_analysis(user_prompt, file_path)

    print("\n" + "="*80)
    print("ðŸ“Š ANALYSIS RESULT:")
    print("="*80)
    print(result)

    print("\n" + "="*80)
    print("âœ… Analysis completed!")
    print("="*80)
